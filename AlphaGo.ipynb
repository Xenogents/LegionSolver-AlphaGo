{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5p7XFs_QNRj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Very roughly coded ruleset for the game with manually defined pieces, not super important to code this bit cleanly\n",
        "\n",
        "# Finding legal actions takes significant computing power, this dictionary will spit back any previously computed\n",
        "# legal actions. Will only overflow when given larger cases.\n",
        "all_legal_actions = {}\n",
        "class Polyomino():\n",
        "    def __init__(self, h, w, idx):\n",
        "        # 1d Encoding of board makes logic much simpler\n",
        "        self.idx = idx\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.state = np.zeros(h*w)\n",
        "        self.position = 0\n",
        "        # Representation not unique for pieces longer than the width of the board, does not scale but works for our cases\n",
        "        # Currently commented out this set of tetrominoes, used when testing easier cases\n",
        "        # self.pieces = np.array([\n",
        "        #     [0,1,2,w+2], [0,2*w-1,w,2*w], [0,w,w+1,w+2], [0,w,2*w,1], [0,w,1,2], [0,1,w+1,2*w+1], [0,w-2,w-1,w], [0,w,2*w,2*w+1], # L piece\n",
        "        #     [0,1,w+1,w+2], [0,2*w-1,w-1,w], [0,w-1,w,1], [0,w,w+1,2*w+1], # Z piece\n",
        "        #     [0,1,w+1,2], [0,w-1,w,2*w], [0,w-1,w,w+1], [0,w,2*w,w+1], # T piece\n",
        "        #     [0,1,2,3], [0,w,2*w,3*w], [0,1,w,w+1]]) # I and square piece\n",
        "        # self.max_pieces = [2,2,2,2,2]\n",
        "        # self.available_pieces = self.max_pieces.copy()\n",
        "        # self.kernel = [(1,0),(2,0),(3,0),(-2,1),(-1,1),(0,1),(1,1),(2,1),(-1,2),(0,2),(1,2),(0,3)]\n",
        "        # self.flat_kernel = [grid[1]*self.w + grid[0] for grid in self.kernel]\n",
        "\n",
        "        # Sets of pentominoes and all of their rotations/reflections. Small optimization can be made by requiring\n",
        "        # that these representations all start with 0. Other useful precomputed values are just manually coded.\n",
        "        self.pieces = np.array([\n",
        "            [0,1,2,w+2,w+3], [0,w,2*w-1,2*w,3*w-1], [0,1,w+1,w+2,w+3], [0,w-1,2*w-1,3*w-1,w], # Z piece\n",
        "            [0,1,w-2,w-1,w], [0,w,2*w,2*w+1,3*w+1], [0,1,2,w-1,w], [0,w,w+1,2*w+1,3*w+1], # Z piece reflected\n",
        "            [0,1,w+1,2*w+1,2*w+2], [0,w-2,2*w-2,w-1,w], [0,2*w-1,w,2*w,1], [0,w,w+1,w+2,2*w+2], # S piece\n",
        "            [0,w,2*w,w+1,w+2], [0,1,w+1,2*w+1,2], [0,w-2,w-1,w,2*w], [0,2*w-1,w,2*w,2*w+1], # T piece\n",
        "            [0,1,2,3,4], [0,w,2*w,3*w,4*w], [0,w-1,w,2*w,w+1]]) # I and cross piece\n",
        "        self.max_pieces = [2,2,2,6,4]\n",
        "        self.available_pieces = self.max_pieces.copy()\n",
        "        self.kernel = [(1,0),(2,0),(3,0),(4,0),(-2,1),(-1,1),(0,1),(1,1),(2,1),(3,1),\n",
        "                        (-2,2),(-1,2),(0,2),(1,2),(2,2),(-1,3),(0,3),(1,3),(0,4)]\n",
        "        self.flat_kernel = [grid[1]*self.w + grid[0] for grid in self.kernel]\n",
        "\n",
        "        # Retrieve correct piece for a given action and vice versa\n",
        "        self.piece_divider = {0:0, 1:8, 2:12, 3:16, 4:18, 5:19}\n",
        "        self.piece_from_action = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:1,9:1,10:1,11:1,12:2,13:2,14:2,15:2,16:3,17:3,18:4}\n",
        "\n",
        "    # Get all possible actions from a given state\n",
        "    def get_legal_actions(self):\n",
        "        state_piece = tuple(np.concatenate([self.state, self.available_pieces]))\n",
        "        if state_piece in all_legal_actions:\n",
        "            return all_legal_actions[state_piece]\n",
        "\n",
        "        actions = []\n",
        "        available_pieces = []\n",
        "        prev_position = self.position\n",
        "        for i, num_pieces in enumerate(self.available_pieces):\n",
        "            if num_pieces != 0:\n",
        "                available_pieces.extend(self.pieces[self.piece_divider[i] : self.piece_divider[i+1]])\n",
        "        available_pieces = set(tuple(piece) for piece in available_pieces)\n",
        "        legal_grids = self.generate_legal_grids()\n",
        "        while (self.position < self.h*self.w) and (self.state[self.position] == 1 or\n",
        "                    (actions := [(i, self.position) for i, piece in enumerate(self.pieces)\n",
        "                        if tuple(piece) in available_pieces and all(x in legal_grids for x in piece[1:])]) == []):\n",
        "            self.position += 1\n",
        "            legal_grids = self.generate_legal_grids()\n",
        "\n",
        "        self.position = prev_position\n",
        "        all_legal_actions[state_piece] = actions\n",
        "        return actions\n",
        "\n",
        "    # Checks the general area around where the piece is to be placed and returns which grids are free\n",
        "    def generate_legal_grids(self):\n",
        "        x = self.position % self.w\n",
        "        y = self.position // self.w\n",
        "        kernel = [self.flat_kernel[i] for i, grid in enumerate(self.kernel)\n",
        "                  if 0 <= grid[0]+x < self.w and grid[1]+y < self.h and self.state[self.position + self.flat_kernel[i]] == 0]\n",
        "        return kernel\n",
        "\n",
        "    # Place a piece\n",
        "    def step(self, action):\n",
        "        self.position = action[1]\n",
        "        self.state[self.pieces[action[0]] + self.position] = 1\n",
        "        self.available_pieces[self.piece_from_action[action[0]]] -= 1\n",
        "\n",
        "    # Unplace a piece\n",
        "    def undo(self, action):\n",
        "        self.state[self.pieces[action[0]] + self.position] = 0\n",
        "        self.position = action[1]\n",
        "        self.available_pieces[self.piece_from_action[action[0]]] += 1\n",
        "\n",
        "    # Resulting value of the game, only called once it has been determined that there are no legal actions remaining\n",
        "    def value(self):\n",
        "        value = np.sum(self.state)\n",
        "        while (self.position < self.h*self.w) and (self.state[self.position] == 1):\n",
        "            self.position += 1\n",
        "        # Place a remaining piece allowing for overlaps or protruding edges to get a more continuous value function\n",
        "        if self.position < self.h*self.w:\n",
        "            piece = np.argmax(np.array(self.available_pieces))\n",
        "            max_grids_covered = 0\n",
        "            for action in self.pieces[self.piece_divider[piece] : self.piece_divider[piece+1]]:\n",
        "                grids_covered = 0\n",
        "                left_x = 0\n",
        "                for i, grid in enumerate(action):\n",
        "                    if grid + self.position < self.h*self.w:\n",
        "                        offset = self.position % self.w\n",
        "                        if i == 0 and grid > 0:\n",
        "                            left_x = grid % self.w - self.w\n",
        "                            if left_x + offset >= 0 and self.state[grid + self.position] == 0:\n",
        "                                grids_covered += 1\n",
        "                        else:\n",
        "                            if grid % self.w + offset < self.w and self.state[grid + self.position] == 0:\n",
        "                                grids_covered += 1\n",
        "                if grids_covered > max_grids_covered:\n",
        "                    max_grids_covered = grids_covered\n",
        "            value += max_grids_covered\n",
        "        return value / (self.h*self.w)\n",
        "\n",
        "    # Method to reset the game to avoid creating too many game instances\n",
        "    def reset_game(self):\n",
        "        self.state = np.zeros(self.h*self.w)\n",
        "        self.position = 0\n",
        "        self.available_pieces = self.max_pieces.copy()\n",
        "\n",
        "    def get_board(self):\n",
        "        return np.reshape(self.state.copy(), (self.h, self.w,1))\n",
        "\n",
        "    def get_available_pieces(self):\n",
        "        return self.available_pieces.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LZRN8cIffqh"
      },
      "outputs": [],
      "source": [
        "# Monte Carlo Tree Search in this code is modified such that it can play any specified number of games in tandem.\n",
        "# Code becomes more complex to understand but saves significant time on computation\n",
        "\n",
        "# Root node of tree search is defined differently in order to add Dirichlet noise and because it has no parents\n",
        "class RootNode:\n",
        "    def __init__(self, priors, legal_actions):\n",
        "        self.parent = None\n",
        "        self.children = None\n",
        "        self.children_priors = None\n",
        "        self.children_values = None\n",
        "        self.children_visits = None\n",
        "        self.children_legal_actions = None\n",
        "        self.visits = 0\n",
        "        self.priors = priors\n",
        "        self.noise = []\n",
        "\n",
        "    # Model assumes all actions are possible at all points in time, must be normalized before using\n",
        "    def get_normalized_priors(self):\n",
        "        priors = self.priors[self.children_legal_actions]\n",
        "        priors = priors / np.sum(priors)\n",
        "        if len(self.noise) == 0: self.noise = np.random.dirichlet([0.03]*len(priors))\n",
        "        if len(priors) > 1: priors = priors = 0.75*priors + 0.25*self.noise\n",
        "        return priors\n",
        "\n",
        "# Information in tree is stored in a somewhat roundabout way, in order to access certain pieces of information about\n",
        "# your current node, you have to go to its parent and then that parent has information about all its children\n",
        "# Unsure if efficient but the code I was referencing from had this node structure so I left it as is\n",
        "class Node:\n",
        "    def __init__(self, idx, parent, action):\n",
        "        self.children = None\n",
        "        self.children_priors = None\n",
        "        self.children_values = None\n",
        "        self.children_visits = None\n",
        "        self.children_legal_actions = None\n",
        "        self.idx = idx\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "\n",
        "    def get_normalized_priors(self):\n",
        "        priors = self.priors[self.children_legal_actions]\n",
        "        return priors / np.sum(priors)\n",
        "    @property\n",
        "    def priors(self):\n",
        "        return self.parent.children_priors[self.idx]\n",
        "    @priors.setter\n",
        "    def priors(self, x):\n",
        "        self.parent.children_priors[self.idx] = x\n",
        "    @property\n",
        "    def visits(self):\n",
        "        return self.parent.children_visits[self.idx]\n",
        "    @visits.setter\n",
        "    def visits(self, x):\n",
        "        self.parent.children_visits[self.idx] = x\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.parent.children_values[self.idx]\n",
        "    @value.setter\n",
        "    def value(self, x):\n",
        "        self.parent.children_values[self.idx] = x\n",
        "\n",
        "# Choosing function for search tree, exploration constant c can be modified freely\n",
        "def get_ucb_scores(node, c=0.1):\n",
        "    priors = node.get_normalized_priors()\n",
        "    return node.children_values + c*priors*node.visits**0.5 / (node.children_visits+1)\n",
        "\n",
        "# Traverse the tree until you reach an unexpanded node\n",
        "def select(root, game):\n",
        "    current = root\n",
        "    while current.children is not None:\n",
        "        ucb_scores = get_ucb_scores(current)\n",
        "        current = current.children[np.argmax(ucb_scores)]\n",
        "        game.step(current.action)\n",
        "    return current\n",
        "\n",
        "def expand(games, leaves, model, children_actions):\n",
        "    states = []\n",
        "    pieces = []\n",
        "    # Record all the states for a single action taken in each direction, this way a call to the model to perform a prediction\n",
        "    # only needs to be performed a single time.\n",
        "    for i, game in enumerate(games):\n",
        "        leaves[i].children_legal_actions = [action[0] for action in children_actions[i]]\n",
        "        for action in children_actions[i]:\n",
        "            position = game.position\n",
        "            game.step(action)\n",
        "            states.append(game.get_board())\n",
        "            pieces.append(game.get_available_pieces())\n",
        "            game.undo((action[0], position))\n",
        "    states = tf.convert_to_tensor(np.array(states), dtype=\"float32\")\n",
        "    pieces = tf.convert_to_tensor(np.array(pieces), dtype=\"float32\")\n",
        "\n",
        "    children_priors, children_values = model(states, pieces)\n",
        "    children_values = children_values[:,0]\n",
        "    # Cannot embed multiple dimensions into prediction batch, so we must seperate the dimensions manually\n",
        "    num_branches = np.cumsum([0]+[len(children_actions[i]) for i in range(len(games))])\n",
        "    for i, leaf in enumerate(leaves):\n",
        "        leaf.children = [Node(idx, leaf, action) for idx, action in enumerate(children_actions[i])]\n",
        "        leaf.children_priors = children_priors.numpy()[num_branches[i]:num_branches[i+1]]\n",
        "        leaf.children_values = children_values.numpy()[num_branches[i]:num_branches[i+1]]\n",
        "        leaf.children_visits = np.zeros(len(children_actions[i]))\n",
        "\n",
        "def backpropagate(leaf, value):\n",
        "    current = leaf\n",
        "    while current.parent is not None:\n",
        "        current.value = (current.value * current.visits + value) / (current.visits + 1)  # incremental mean update\n",
        "        current.visits += 1\n",
        "        current = current.parent\n",
        "    current.visits += 1\n",
        "\n",
        "# Monte Carlo Tree search\n",
        "def search(games, model, iterations):\n",
        "    states = tf.convert_to_tensor(np.array([game.get_board() for game in games]), dtype=\"float32\")\n",
        "    pieces = tf.convert_to_tensor(np.array([game.get_available_pieces() for game in games]), dtype=\"float32\")\n",
        "    priors = model(states, pieces)[0].numpy()\n",
        "    roots = [RootNode(priors[i], games[i].get_legal_actions()) for i in range(len(games))]\n",
        "\n",
        "    positions = [game.position for game in games]\n",
        "    states = [game.state.copy() for game in games]\n",
        "    available_pieces = [game.available_pieces.copy() for game in games]\n",
        "    for _ in range(iterations):\n",
        "        leaves = np.array([select(roots[i], games[i]) for i in range(len(games))])\n",
        "        children_actions = [games[i].get_legal_actions() for i in range(len(games))]\n",
        "        while (unfinished_games:=[i for i in range(len(games)) if children_actions[i] != []]) != []:\n",
        "            expand(games[unfinished_games], leaves[unfinished_games], model,\n",
        "                            [actions for actions in children_actions if actions != []])\n",
        "            children_actions = [[] for _ in range(len(games))]\n",
        "            for game_idx in unfinished_games:\n",
        "                leaves[game_idx] = leaves[game_idx].children[np.argmax(leaves[game_idx].children_values)]\n",
        "                games[game_idx].step(leaves[game_idx].action)\n",
        "                children_actions[game_idx] = games[game_idx].get_legal_actions()\n",
        "        for i, game in enumerate(games):\n",
        "            backpropagate(leaves[i], game.value())\n",
        "            game.position = positions[i]\n",
        "            game.state = states[i].copy()\n",
        "            game.available_pieces = available_pieces[i].copy()\n",
        "    return roots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM5sL_NpfBEL"
      },
      "outputs": [],
      "source": [
        "ITERATIONS = 100 # Number of times to swap between self-play and neural net training\n",
        "SELFPLAY_GAMES = 40 # Number of self-play games before training the neural net on a minibatch\n",
        "SEARCH_ITERATIONS = 50 # Number of whole game simulations per tree\n",
        "BATCH_SIZE = 256 # Number of states to train on per epoch\n",
        "height = 8\n",
        "width = 10\n",
        "games = [Polyomino(height, width, idx) for idx in range(SELFPLAY_GAMES)]\n",
        "action_size = len(games[0].pieces)\n",
        "\n",
        "# Model takes in the number of remaining pieces as input once the convolution layers are finished\n",
        "input1 = tf.keras.layers.Input(shape=(height,width,1))\n",
        "input2 = tf.keras.layers.Input(shape=(5))\n",
        "conv1 = tf.keras.layers.Conv2D(64,(3,3), activation = 'relu', padding = 'same')(input1)\n",
        "conv1 = tf.keras.layers.Conv2D(128,(3,3), activation = 'relu', padding = 'same')(conv1)\n",
        "conv1 = tf.keras.layers.Conv2D(256,(3,3), activation = 'relu', padding = 'same')(conv1)\n",
        "flat = tf.keras.layers.Flatten()(conv1)\n",
        "flat2 = tf.keras.layers.Dense(128, activation=\"relu\")(flat)\n",
        "merged = tf.keras.layers.Concatenate()([flat2, input2])\n",
        "dense = tf.keras.layers.Dense(128, activation=\"relu\")(merged)\n",
        "output1 = tf.keras.layers.Dense(action_size, activation=\"softmax\", name='pi')(dense)\n",
        "dense = tf.keras.layers.Dense(128, activation=\"relu\")(dense)\n",
        "output2= tf.keras.layers.Dense(1, activation=\"sigmoid\", name='v',\n",
        "                               kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.1))(dense)\n",
        "model = tf.keras.models.Model(inputs=[input1,input2], outputs=[output1, output2])\n",
        "model.compile(loss=['categorical_crossentropy', \"mean_squared_error\"], loss_weights=[1., 100.],\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
        "\n",
        "examples = []\n",
        "avg_values = []\n",
        "history = []\n",
        "waiting_for_result = []\n",
        "for _ in tqdm(range(ITERATIONS)):\n",
        "    # Conversion to a frozen function increases prediction efficiency\n",
        "    full_model = tf.function(lambda x: model(x))\n",
        "    full_model = full_model.get_concrete_function([tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype),\n",
        "                                                   tf.TensorSpec(model.inputs[1].shape, model.inputs[1].dtype)])\n",
        "    frozen_func = convert_variables_to_constants_v2(full_model)\n",
        "    frozen_func.graph.as_graph_def()\n",
        "\n",
        "    while (unfinished_games:=[game for game in games if game.get_legal_actions() != []]) != []:\n",
        "        roots = search(np.array(unfinished_games), frozen_func, SEARCH_ITERATIONS)\n",
        "        for idx, root in enumerate(roots):\n",
        "            pi_example = root.children_visits\n",
        "            actions = [child.action for child in root.children]\n",
        "\n",
        "            # In order to train the model, we have to put back the invalid actions we took out\n",
        "            # during tree search and give them a value of 0\n",
        "            invalid_actions = [i for i in range(action_size) if i not in [action[0] for action in actions]]\n",
        "            for action in invalid_actions:\n",
        "                if action < len(pi_example):\n",
        "                    pi_example = np.insert(pi_example, action, 0)\n",
        "                else:\n",
        "                    pi_example = np.append(pi_example, 0)\n",
        "\n",
        "            waiting_for_result.append((unfinished_games[idx].get_board(), pi_example/np.sum(pi_example),\n",
        "                                       unfinished_games[idx].get_available_pieces(), unfinished_games[idx].idx))\n",
        "            # Stochastically determine the next piece to place\n",
        "            choice = random.choices(population=[i for i in range(len(actions))],\n",
        "                                    weights=root.children_visits/np.sum(root.children_visits))[0]\n",
        "            action = root.children[choice].action\n",
        "            unfinished_games[idx].step(action)\n",
        "\n",
        "    # Resulting game outcome of each state is not known until the games are finished\n",
        "    examples += [(s, a, p, games[idx].value()) for s, a, p, idx in waiting_for_result]\n",
        "    waiting_for_result = []\n",
        "    for i in range(SELFPLAY_GAMES):\n",
        "        games[i].reset_game()\n",
        "\n",
        "    # Model is trained on most recent 3000 states\n",
        "    recent_examples = examples[-3000:].copy()\n",
        "    random.shuffle(recent_examples)\n",
        "    states = np.reshape(np.array([example[0] for example in recent_examples[0:BATCH_SIZE]]), (BATCH_SIZE,height,width,1))\n",
        "    actions = np.array([example[1] for example in recent_examples[0:BATCH_SIZE]])\n",
        "    pieces = np.array([example[2] for example in recent_examples[0:BATCH_SIZE]])\n",
        "    values = np.array([example[3] for example in recent_examples[0:BATCH_SIZE]])\n",
        "    print(values)\n",
        "    avg_values.append(np.mean(np.array([example[3] for example in recent_examples])))\n",
        "    if len(examples) >= 3000:\n",
        "        history.append(model.fit([states, pieces], [actions, values], epochs=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Play games manually after model is trained\n",
        "games = [Polyomino(height, width, idx) for idx in range(1)]\n",
        "game = games[0]\n",
        "# game.step((12,0))\n",
        "# game.step((0,1))\n",
        "# game.step((2,4))\n",
        "# game.step((8,7))\n",
        "# game.step((15,21))\n",
        "# game.step((11,22))\n",
        "# game.step((17,19))\n",
        "# game.step((12,24))\n",
        "# game.step((17,30))\n",
        "# game.step((18,34))\n",
        "# game.step((8,35))\n",
        "# game.step((8,47))\n",
        "# game.step((10,51))\n",
        "# game.step((14,57))\n",
        "root = search(np.array(games), frozen_func, 500)[0]\n",
        "print(root.children_values)\n",
        "pi_example = root.children_visits\n",
        "actionss = [child.action for child in root.children]\n",
        "invalid_actions = [i for i in range(action_size) if i not in [action[0] for action in actionss]]\n",
        "for action in invalid_actions:\n",
        "    if action < len(pi_example):\n",
        "        pi_example = np.insert(pi_example, action, 0)\n",
        "    else:\n",
        "        pi_example = np.append(pi_example, 0)\n",
        "print(pi_example)\n",
        "print(root.get_normalized_priors())\n",
        "print(model([np.reshape(game.get_board(), (1,8,10,1)), np.reshape(game.get_available_pieces(), (1,5))]))\n",
        "\n",
        "print(np.reshape(game.get_board(), (8,10)))\n",
        "print(game.get_legal_actions())"
      ],
      "metadata": {
        "id": "5OzICA4hZhoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqmMilpfRf41"
      },
      "outputs": [],
      "source": [
        "# Section to visualize results post-training\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "data1 = [1 - value for value in avg_values]\n",
        "data2 = [loss.history[\"loss\"][0] for loss in history]\n",
        "t = [i for i in range(len(data1))]\n",
        "t2 = [i for i in range(len(data1)-len(data2),len(data1))]\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel('Iterations')\n",
        "ax1.set_ylabel('1 - Game Result', color='red')\n",
        "# ax1.set_ylim(0,0.65)\n",
        "ax1.plot(t, data1, color='red')\n",
        "ax1.tick_params(axis='y', labelcolor='red')\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.set_ylabel('Network Loss', color=\"blue\")\n",
        "ax2.plot(t2, data2, color=\"blue\")\n",
        "ax2.tick_params(axis='y', labelcolor=\"blue\")\n",
        "# plt.title(\"Training results over 8x10 board\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}