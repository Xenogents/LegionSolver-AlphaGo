{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM5sL_NpfBEL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from Polyomino import Polyomino\n",
        "from MCTS import search\n",
        "\n",
        "ITERATIONS = 100 # Number of times to swap between self-play and neural net training\n",
        "SELFPLAY_GAMES = 40 # Number of self-play games before training the neural net on a minibatch\n",
        "SEARCH_ITERATIONS = 50 # Number of whole game simulations per tree\n",
        "BATCH_SIZE = 256 # Number of states to train on per epoch\n",
        "height = 8\n",
        "width = 10\n",
        "games = [Polyomino(height, width, idx) for idx in range(SELFPLAY_GAMES)]\n",
        "action_size = len(games[0].pieces)\n",
        "\n",
        "# Model takes in the number of remaining pieces as input once the convolution layers are finished\n",
        "input1 = tf.keras.layers.Input(shape=(height,width,1))\n",
        "input2 = tf.keras.layers.Input(shape=(5))\n",
        "conv1 = tf.keras.layers.Conv2D(64,(3,3), activation = 'relu', padding = 'same')(input1)\n",
        "conv1 = tf.keras.layers.Conv2D(128,(3,3), activation = 'relu', padding = 'same')(conv1)\n",
        "conv1 = tf.keras.layers.Conv2D(256,(3,3), activation = 'relu', padding = 'same')(conv1)\n",
        "flat = tf.keras.layers.Flatten()(conv1)\n",
        "flat2 = tf.keras.layers.Dense(128, activation=\"relu\")(flat)\n",
        "merged = tf.keras.layers.Concatenate()([flat2, input2])\n",
        "dense = tf.keras.layers.Dense(128, activation=\"relu\")(merged)\n",
        "output1 = tf.keras.layers.Dense(action_size, activation=\"softmax\", name='pi')(dense)\n",
        "dense = tf.keras.layers.Dense(128, activation=\"relu\")(dense)\n",
        "output2= tf.keras.layers.Dense(1, activation=\"sigmoid\", name='v',\n",
        "                               kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.1))(dense)\n",
        "model = tf.keras.models.Model(inputs=[input1,input2], outputs=[output1, output2])\n",
        "model.compile(loss=['categorical_crossentropy', \"mean_squared_error\"], loss_weights=[1., 100.],\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
        "\n",
        "examples = []\n",
        "avg_values = []\n",
        "history = []\n",
        "waiting_for_result = []\n",
        "for _ in tqdm(range(ITERATIONS)):\n",
        "    # Conversion to a frozen function increases prediction efficiency\n",
        "    full_model = tf.function(lambda x: model(x))\n",
        "    full_model = full_model.get_concrete_function([tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype),\n",
        "                                                   tf.TensorSpec(model.inputs[1].shape, model.inputs[1].dtype)])\n",
        "    frozen_func = convert_variables_to_constants_v2(full_model)\n",
        "    frozen_func.graph.as_graph_def()\n",
        "\n",
        "    while (unfinished_games:=[game for game in games if game.get_legal_actions() != []]) != []:\n",
        "        roots = search(np.array(unfinished_games), frozen_func, SEARCH_ITERATIONS)\n",
        "        for idx, root in enumerate(roots):\n",
        "            pi_example = root.children_visits\n",
        "            actions = [child.action for child in root.children]\n",
        "\n",
        "            # In order to train the model, we have to put back the invalid actions we took out\n",
        "            # during tree search and give them a value of 0\n",
        "            invalid_actions = [i for i in range(action_size) if i not in [action[0] for action in actions]]\n",
        "            for action in invalid_actions:\n",
        "                if action < len(pi_example):\n",
        "                    pi_example = np.insert(pi_example, action, 0)\n",
        "                else:\n",
        "                    pi_example = np.append(pi_example, 0)\n",
        "\n",
        "            waiting_for_result.append((unfinished_games[idx].get_board(), pi_example/np.sum(pi_example),\n",
        "                                       unfinished_games[idx].get_available_pieces(), unfinished_games[idx].idx))\n",
        "            # Stochastically determine the next piece to place\n",
        "            choice = random.choices(population=[i for i in range(len(actions))],\n",
        "                                    weights=root.children_visits/np.sum(root.children_visits))[0]\n",
        "            action = root.children[choice].action\n",
        "            unfinished_games[idx].step(action)\n",
        "\n",
        "    # Resulting game outcome of each state is not known until the games are finished\n",
        "    examples += [(s, a, p, games[idx].value()) for s, a, p, idx in waiting_for_result]\n",
        "    waiting_for_result = []\n",
        "    for i in range(SELFPLAY_GAMES):\n",
        "        games[i].reset_game()\n",
        "\n",
        "    # Model is trained on most recent 3000 states\n",
        "    recent_examples = examples[-3000:].copy()\n",
        "    random.shuffle(recent_examples)\n",
        "    states = np.reshape(np.array([example[0] for example in recent_examples[0:BATCH_SIZE]]), (BATCH_SIZE,height,width,1))\n",
        "    actions = np.array([example[1] for example in recent_examples[0:BATCH_SIZE]])\n",
        "    pieces = np.array([example[2] for example in recent_examples[0:BATCH_SIZE]])\n",
        "    values = np.array([example[3] for example in recent_examples[0:BATCH_SIZE]])\n",
        "    # print(values)\n",
        "    avg_values.append(np.mean(np.array([example[3] for example in recent_examples])))\n",
        "    if len(examples) >= 3000:\n",
        "        history.append(model.fit([states, pieces], [actions, values], epochs=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OzICA4hZhoR"
      },
      "outputs": [],
      "source": [
        "# Play games manually after model is trained\n",
        "games = [Polyomino(height, width, idx) for idx in range(1)]\n",
        "game = games[0]\n",
        "# game.step((12,0))\n",
        "# game.step((0,1))\n",
        "# game.step((2,4))\n",
        "# game.step((8,7))\n",
        "# game.step((15,21))\n",
        "# game.step((11,22))\n",
        "# game.step((17,19))\n",
        "# game.step((12,24))\n",
        "# game.step((17,30))\n",
        "# game.step((18,34))\n",
        "# game.step((8,35))\n",
        "# game.step((8,47))\n",
        "# game.step((10,51))\n",
        "# game.step((14,57))\n",
        "root = search(np.array(games), frozen_func, 500)[0]\n",
        "print(root.children_values)\n",
        "pi_example = root.children_visits\n",
        "actionss = [child.action for child in root.children]\n",
        "invalid_actions = [i for i in range(action_size) if i not in [action[0] for action in actionss]]\n",
        "for action in invalid_actions:\n",
        "    if action < len(pi_example):\n",
        "        pi_example = np.insert(pi_example, action, 0)\n",
        "    else:\n",
        "        pi_example = np.append(pi_example, 0)\n",
        "print(pi_example)\n",
        "print(root.get_normalized_priors())\n",
        "print(model([np.reshape(game.get_board(), (1,8,10,1)), np.reshape(game.get_available_pieces(), (1,5))]))\n",
        "\n",
        "print(np.reshape(game.get_board(), (8,10)))\n",
        "print(game.get_legal_actions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqmMilpfRf41"
      },
      "outputs": [],
      "source": [
        "# Section to visualize results post-training\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [6.4, 4.8]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "data1 = [1 - value for value in avg_values]\n",
        "data2 = [loss.history[\"loss\"][0] for loss in history]\n",
        "t = [i for i in range(len(data1))]\n",
        "t2 = [i for i in range(len(data1)-len(data2),len(data1))]\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel('Iterations')\n",
        "ax1.set_ylabel('1 - Game Result', color='red')\n",
        "# ax1.set_ylim(0,0.65)\n",
        "ax1.plot(t, data1, color='red')\n",
        "ax1.tick_params(axis='y', labelcolor='red')\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.set_ylabel('Network Loss', color=\"blue\")\n",
        "ax2.plot(t2, data2, color=\"blue\")\n",
        "ax2.tick_params(axis='y', labelcolor=\"blue\")\n",
        "plt.title(\"Training results over 8x10 board\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
